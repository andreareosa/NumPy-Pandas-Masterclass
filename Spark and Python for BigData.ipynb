{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e102295",
   "metadata": {},
   "source": [
    "### **Spark DataFrame Basics**\n",
    "\n",
    "Spark DataFrames are the workhouse and main way of working with Spark and Python post Spark 2.0. DataFrames act as powerful versions of tables, with rows and columns, easily handling large datasets. The shift to DataFrames provides many advantages:\n",
    "\n",
    "- A much simpler syntax\n",
    "- Ability to use SQL directly in the dataframe\n",
    "- Operations are automatically distributed across RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "The main advantage to using Spark DataFrames vs those other programs is that Spark can handle data across many RDDs, huge data sets that would never fit on a single computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f572e5",
   "metadata": {},
   "source": [
    "**Create a Spark Session**\n",
    "\n",
    "In order to actually work with Spark the first thing we need to do is **to create a spark session**\n",
    "\n",
    "Spark session is a unified entry point of a spark application from Spark 2.0. It provides a way to interact with various spark's functionality with a lesser number of constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67693910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['JAVA_HOME'] = 'C:\\Java\\jre1.8.0_351' #to fix a problem I had with  nvironment variables\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c2ed3",
   "metadata": {},
   "source": [
    "#### **Read in a dataframe from a JSON file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bc12ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\CTW02528\\OneDrive - Critical Techworks\\Desktop\\Data Engineer CTW\\4 - Trainning\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\people.json'\n",
    "\n",
    "df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "040a458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6effc857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "015c1f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6edb10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a263b0",
   "metadata": {},
   "source": [
    "#### **Manually set the dataframe schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34c3fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (StructField,StringType,\n",
    "                               IntegerType,StructType)\n",
    "\n",
    "data_schema = [StructField('age',IntegerType(),True),\n",
    "               StructField('name',StringType(),True)] #True means that the column may have NULL values\n",
    "\n",
    "final_struct = StructType(fields=data_schema)\n",
    "\n",
    "df = spark.read.json(path,schema = final_struct)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f652847",
   "metadata": {},
   "source": [
    "#### **Grab data from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebfbff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| Age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a80eb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8b6ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, name: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['age','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7297511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|newage|\n",
      "+----+-------+------+\n",
      "|null|Michael|  null|\n",
      "|  30|   Andy|    60|\n",
      "|  19| Justin|    38|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a double age column\n",
    "df.withColumn('newage',df['age'] * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab1f4425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|new_age|   name|\n",
      "+-------+-------+\n",
      "|   null|Michael|\n",
      "|     30|   Andy|\n",
      "|     19| Justin|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename a column\n",
    "df = df.withColumnRenamed('age','new_age')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23245136",
   "metadata": {},
   "source": [
    "#### **Use SQL to directly deal and interact with the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42fa714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|new_age|name|\n",
      "+-------+----+\n",
      "|     30|Andy|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a sql temporary view\n",
    "df.createOrReplaceTempView('people')\n",
    "query = spark.sql('SELECT * \\\n",
    "                  FROM people \\\n",
    "                  WHERE new_age = 30').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff3ff0",
   "metadata": {},
   "source": [
    "#### **Basic Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f62431e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\CTW02528\\OneDrive - Critical Techworks\\Desktop\\Data Engineer CTW\\4 - Trainning\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\appl_stock.csv'\n",
    "\n",
    "df = spark.read.csv(path,header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4de598",
   "metadata": {},
   "source": [
    "#### **Filter data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "096ecf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|        213.429998|\n",
      "|        214.599998|\n",
      "|        214.379993|\n",
      "|            211.75|\n",
      "|        210.299994|\n",
      "|212.79999700000002|\n",
      "|209.18999499999998|\n",
      "|        207.870005|\n",
      "|210.11000299999998|\n",
      "|210.92999500000002|\n",
      "|        208.330002|\n",
      "|        214.910006|\n",
      "|        212.079994|\n",
      "|206.78000600000001|\n",
      "|202.51000200000001|\n",
      "|205.95000100000001|\n",
      "|        206.849995|\n",
      "|        204.930004|\n",
      "|        201.079996|\n",
      "|192.36999699999998|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the opening price for every row that has a closing price less than 500€, using SQL syntax\n",
    "df.filter(\"Close < 500\").select('Open').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44ecb2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|        213.429998|\n",
      "|        214.599998|\n",
      "|        214.379993|\n",
      "|            211.75|\n",
      "|        210.299994|\n",
      "|212.79999700000002|\n",
      "|209.18999499999998|\n",
      "|        207.870005|\n",
      "|210.11000299999998|\n",
      "|210.92999500000002|\n",
      "|        208.330002|\n",
      "|        214.910006|\n",
      "|        212.079994|\n",
      "|206.78000600000001|\n",
      "|202.51000200000001|\n",
      "|205.95000100000001|\n",
      "|        206.849995|\n",
      "|        204.930004|\n",
      "|        201.079996|\n",
      "|192.36999699999998|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the same using dataframes syntax\n",
    "df.filter(df['Close'] < 500).select('Open').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f166e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|192.36999699999998|\n",
      "|        195.909998|\n",
      "|        195.169994|\n",
      "|        196.730003|\n",
      "|192.63000300000002|\n",
      "|        195.690006|\n",
      "|        196.419996|\n",
      "|        195.889997|\n",
      "|        194.880001|\n",
      "|        198.109995|\n",
      "|        199.999998|\n",
      "|        198.229998|\n",
      "|        197.380005|\n",
      "|         92.699997|\n",
      "|         94.730003|\n",
      "|         94.129997|\n",
      "|         94.040001|\n",
      "|         92.199997|\n",
      "|         91.510002|\n",
      "|         92.309998|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering using multiple condition where the close are less than 200 and open are not greater than 200\n",
    "df.filter( (df['Close'] < 500) & ~(df['Open'] > 200) ).select('Open').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cc833ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save as a variable the result where the low price is equal to 197.16\n",
    "result = df.filter(df['low'] == 197.16).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6e069d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2010, 1, 22, 0, 0),\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb7c62",
   "metadata": {},
   "source": [
    "#### **GroupBy and Aggregate Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c86573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\CTW02528\\OneDrive - Critical Techworks\\Desktop\\Data Engineer CTW\\4 - Trainning\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\sales_info.csv'\n",
    "\n",
    "df = spark.read.csv(path,header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1a5742a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the average Sales according to the company\n",
    "df.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8c4daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return total sales \n",
    "df.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29ab8a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Max sales per comapny\n",
    "df.groupBy('Company').agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7ed573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct,avg,stddev,mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19347b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Distinct Companies|\n",
      "+------------------+\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct companies\n",
    "df.select(countDistinct('Company').alias('Distinct Companies')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2c319e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order by Ascending sales\n",
    "df.orderBy('sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82191d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descending call off the column itself.\n",
    "df.orderBy(df['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4ab55",
   "metadata": {},
   "source": [
    "#### **Missing Data**\n",
    "\n",
    "Often data sources are incomplete, which means we will have missing data, we have 3 basic options for filling in missing data:\n",
    "\n",
    "* Just keep the missing data points.\n",
    "* Drop the missing data points (including the entire row)\n",
    "* Fill them in with some other value as the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1f0fd0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\CTW02528\\OneDrive - Critical Techworks\\Desktop\\Data Engineer CTW\\4 - Trainning\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\ContainsNull.csv'\n",
    "\n",
    "df = spark.read.csv(path,header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f979e0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "602d70a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop only rows that have at least 2 non null values\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1e8eb526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows if all rows are null\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "062fb767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2cf88dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+\n",
      "|  Id|       Name|Sales|\n",
      "+----+-----------+-----+\n",
      "|emp1|       John| null|\n",
      "|emp2|FILL NUMBER| null|\n",
      "|emp3|FILL NUMBER|345.0|\n",
      "|emp4|      Cindy|456.0|\n",
      "+----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark is smart to not fill Sales, a numerical field, with a string\n",
    "df.na.fill('FILL NUMBER').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c36d9c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We should specify the column to fill\n",
    "df.na.fill('No Name',subset='Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dec2e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values for Sales with the average Sales\n",
    "\n",
    "mean_val = df.select(avg(df['sales'])).collect(\n",
    "mean_sales = mean_val[0].asDict()['avg(sales)']\n",
    "\n",
    "\n",
    "df.na.fill(mean_sales,subset='Sales').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b218c6",
   "metadata": {},
   "source": [
    "#### **Dates and Timestamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9859a30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = r'C:\\Users\\CTW02528\\OneDrive - Critical Techworks\\Desktop\\Data Engineer CTW\\4 - Trainning\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\appl_stock.csv'\n",
    "\n",
    "df = spark.read.csv(path,header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f8b6a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth,hour,\n",
    "                                   dayofyear,month,\n",
    "                                   year,weekofyear,\n",
    "                                   format_number,date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3bde0e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+\n",
      "|               Date|Day Of Month|Month|\n",
      "+-------------------+------------+-----+\n",
      "|2010-01-04 00:00:00|           4|    1|\n",
      "|2010-01-05 00:00:00|           5|    1|\n",
      "|2010-01-06 00:00:00|           6|    1|\n",
      "|2010-01-07 00:00:00|           7|    1|\n",
      "|2010-01-08 00:00:00|           8|    1|\n",
      "|2010-01-11 00:00:00|          11|    1|\n",
      "|2010-01-12 00:00:00|          12|    1|\n",
      "|2010-01-13 00:00:00|          13|    1|\n",
      "|2010-01-14 00:00:00|          14|    1|\n",
      "|2010-01-15 00:00:00|          15|    1|\n",
      "|2010-01-19 00:00:00|          19|    1|\n",
      "|2010-01-20 00:00:00|          20|    1|\n",
      "|2010-01-21 00:00:00|          21|    1|\n",
      "|2010-01-22 00:00:00|          22|    1|\n",
      "|2010-01-25 00:00:00|          25|    1|\n",
      "|2010-01-26 00:00:00|          26|    1|\n",
      "|2010-01-27 00:00:00|          27|    1|\n",
      "|2010-01-28 00:00:00|          28|    1|\n",
      "|2010-01-29 00:00:00|          29|    1|\n",
      "|2010-02-01 00:00:00|           1|    2|\n",
      "+-------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Date'\n",
    "           ,dayofmonth(df['Date']).alias('Day Of Month')\n",
    "           ,month(df['Date']).alias('Month')\n",
    "          ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a9497bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2010| 259.8424600000002|\n",
      "|2011|364.00432532142867|\n",
      "|2012| 576.0497195640002|\n",
      "|2013| 472.6348802857143|\n",
      "|2014| 295.4023416507935|\n",
      "|2015|120.03999980555547|\n",
      "|2016|104.60400786904763|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the average closing price per year\n",
    "\n",
    "df = df.withColumn('Year',year(df['date']))\n",
    "\n",
    "result = df.groupBy('Year').agg({'Close':'Mean'}).orderBy('Year')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "01747dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Sales|\n",
      "+----+---------------------+\n",
      "|2010|               259.84|\n",
      "|2011|               364.00|\n",
      "|2012|               576.05|\n",
      "|2013|               472.63|\n",
      "|2014|               295.40|\n",
      "|2015|               120.04|\n",
      "|2016|               104.60|\n",
      "+----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Format avg(close) column and rename column\n",
    "result.select(['Year'\n",
    "               ,format_number('avg(Close)',2).alias('Average Closing Sales')]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
