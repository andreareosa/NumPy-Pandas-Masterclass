# NumPy & Pandas Masterclass

The objetive of this repository is to master the essentials of NumPy and Pandas, two of Python's most powerful data packages


## NumPy

**NumPy** also known as Numerical Python is an open-source library that is the universal standard for working with numerical data in Python, and forms the foundation of other libraries like Pandas as Pandas DataFrames are built on NumPy arrays and can leverage NumPy functions.

- To access my NumPy notebook: [Click here!](https://github.com/andreareosa/NumPy-Pandas-Masterclass/blob/main/NumPy%20Masterclass.ipynb)
- To access NumPy documentation: [Click here!](https://numpy.org/doc/stable/#)

## Pandas

**Pandas** is Python’s most widely used library for data analysis, and contains functions for accessing, aggregating, joining, and analyzing data. 
Its data structure, the DataFrame, is analogous to SQL tables or Excel worksheets


### 1 - Pandas Series

Pandas Series is the equivalent of a column of data. In this section we will cover their basic properties, creation, manipulation and useful functions.
Pandas series are really just numpy arrays with additional features layered into them to make them easier to work with.

- To access my Pandas Series notebook: [Click here!](https://github.com/andreareosa/NumPy-Pandas-Masterclass/blob/main/Pandas%20Series%20Masterclass.ipynb)


### 2 - Pandas DataFranes

Pandas DataFranes is the Python equivalent of an Excel or SQL table which we'll use to store and analyze data. They are a tabular data structure, made up from columns and rows.

- To access my Pandas DataFranes notebook: [Click here!](https://github.com/andreareosa/NumPy-Pandas-Masterclass/blob/main/Pandas%20Dataframes%20Masterclass.ipynb)


## PySpark

**Spark** is a framework which provides parallel and distributed computing on big data. To perform its parallel processing, spark splits the data into smaller chunks (i.e., partitions) and distributes the same data to each node in the cluster to provide a parallel execution of the data. This partitioning of data is performed by spark’s internals and the same can also be controlled by the user. 

**PySpark** is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. To use it, it's good to be familiar with Python and libraries such as Pandas as  PySpark is a good language to learn to create more scalable analyses and pipelines.![image](https://user-images.githubusercontent.com/81584993/212941853-5eef1e4c-feed-4a68-a0a2-5891eaf135a3.png)

- To access my Spark DataFrame Basics notebook: [Click here!](https://github.com/andreareosa/NumPy-Pandas-Masterclass/blob/main/Spark%20and%20Python%20for%20BigData.ipynb)

